Loading dataset to cpu
Using PyTorch automatic mixed-precision
GPU is available: True, Quantity: None
Dataset(s) used:
Dataset length: ('train', 177907)
Dataset length: ('val', 9363)
Dataset length: ('test', 0)
Data(n_atoms=[1], pos=[52, 3], cell=[1, 3, 3], structure_id=[1], z=[52], u=[1, 3], y=[1, 1], forces=[52, 3], stress=[1, 3, 3], x=[52, 100])
tensor(30)
tensor([-236.2635])

--------------------------------------------------------------------------
               Layer.Parameter    Param Tensor Shape              Param #
--------------------------------------------------------------------------
              embedding.weight            [100, 128]                12800
      distance_expansion.means                  [50]                   50
      distance_expansion.betas                  [50]                   50
neighbor_embedding.embedding.weight            [100, 128]                12800
neighbor_embedding.distance_proj.weight             [128, 50]                 6400
neighbor_embedding.distance_proj.bias                 [128]                  128
neighbor_embedding.combine.weight            [128, 256]                32768
neighbor_embedding.combine.bias                 [128]                  128
attention_layers.0.layernorm.weight                 [128]                  128
attention_layers.0.layernorm.bias                 [128]                  128
attention_layers.0.q_proj.weight            [128, 128]                16384
attention_layers.0.q_proj.bias                 [128]                  128
attention_layers.0.k_proj.weight            [128, 128]                16384
attention_layers.0.k_proj.bias                 [128]                  128
attention_layers.0.v_proj.weight            [384, 128]                49152
attention_layers.0.v_proj.bias                 [384]                  384
attention_layers.0.o_proj.weight            [384, 128]                49152
attention_layers.0.o_proj.bias                 [384]                  384
attention_layers.0.vec_proj.weight            [384, 128]                49152
attention_layers.0.dk_proj.weight             [128, 50]                 6400
attention_layers.0.dk_proj.bias                 [128]                  128
attention_layers.0.dv_proj.weight             [384, 50]                19200
attention_layers.0.dv_proj.bias                 [384]                  384
attention_layers.1.layernorm.weight                 [128]                  128
attention_layers.1.layernorm.bias                 [128]                  128
attention_layers.1.q_proj.weight            [128, 128]                16384
attention_layers.1.q_proj.bias                 [128]                  128
attention_layers.1.k_proj.weight            [128, 128]                16384
attention_layers.1.k_proj.bias                 [128]                  128
attention_layers.1.v_proj.weight            [384, 128]                49152
attention_layers.1.v_proj.bias                 [384]                  384
attention_layers.1.o_proj.weight            [384, 128]                49152
attention_layers.1.o_proj.bias                 [384]                  384
attention_layers.1.vec_proj.weight            [384, 128]                49152
attention_layers.1.dk_proj.weight             [128, 50]                 6400
attention_layers.1.dk_proj.bias                 [128]                  128
attention_layers.1.dv_proj.weight             [384, 50]                19200
attention_layers.1.dv_proj.bias                 [384]                  384
attention_layers.2.layernorm.weight                 [128]                  128
attention_layers.2.layernorm.bias                 [128]                  128
attention_layers.2.q_proj.weight            [128, 128]                16384
attention_layers.2.q_proj.bias                 [128]                  128
attention_layers.2.k_proj.weight            [128, 128]                16384
attention_layers.2.k_proj.bias                 [128]                  128
attention_layers.2.v_proj.weight            [384, 128]                49152
attention_layers.2.v_proj.bias                 [384]                  384
attention_layers.2.o_proj.weight            [384, 128]                49152
attention_layers.2.o_proj.bias                 [384]                  384
attention_layers.2.vec_proj.weight            [384, 128]                49152
attention_layers.2.dk_proj.weight             [128, 50]                 6400
attention_layers.2.dk_proj.bias                 [128]                  128
attention_layers.2.dv_proj.weight             [384, 50]                19200
attention_layers.2.dv_proj.bias                 [384]                  384
attention_layers.3.layernorm.weight                 [128]                  128
attention_layers.3.layernorm.bias                 [128]                  128
attention_layers.3.q_proj.weight            [128, 128]                16384
attention_layers.3.q_proj.bias                 [128]                  128
attention_layers.3.k_proj.weight            [128, 128]                16384
attention_layers.3.k_proj.bias                 [128]                  128
attention_layers.3.v_proj.weight            [384, 128]                49152
attention_layers.3.v_proj.bias                 [384]                  384
attention_layers.3.o_proj.weight            [384, 128]                49152
attention_layers.3.o_proj.bias                 [384]                  384
attention_layers.3.vec_proj.weight            [384, 128]                49152
attention_layers.3.dk_proj.weight             [128, 50]                 6400
attention_layers.3.dk_proj.bias                 [128]                  128
attention_layers.3.dv_proj.weight             [384, 50]                19200
attention_layers.3.dv_proj.bias                 [384]                  384
attention_layers.4.layernorm.weight                 [128]                  128
attention_layers.4.layernorm.bias                 [128]                  128
attention_layers.4.q_proj.weight            [128, 128]                16384
attention_layers.4.q_proj.bias                 [128]                  128
attention_layers.4.k_proj.weight            [128, 128]                16384
attention_layers.4.k_proj.bias                 [128]                  128
attention_layers.4.v_proj.weight            [384, 128]                49152
attention_layers.4.v_proj.bias                 [384]                  384
attention_layers.4.o_proj.weight            [384, 128]                49152
attention_layers.4.o_proj.bias                 [384]                  384
attention_layers.4.vec_proj.weight            [384, 128]                49152
attention_layers.4.dk_proj.weight             [128, 50]                 6400
attention_layers.4.dk_proj.bias                 [128]                  128
attention_layers.4.dv_proj.weight             [384, 50]                19200
attention_layers.4.dv_proj.bias                 [384]                  384
attention_layers.5.layernorm.weight                 [128]                  128
attention_layers.5.layernorm.bias                 [128]                  128
attention_layers.5.q_proj.weight            [128, 128]                16384
attention_layers.5.q_proj.bias                 [128]                  128
attention_layers.5.k_proj.weight            [128, 128]                16384
attention_layers.5.k_proj.bias                 [128]                  128
attention_layers.5.v_proj.weight            [384, 128]                49152
attention_layers.5.v_proj.bias                 [384]                  384
attention_layers.5.o_proj.weight            [384, 128]                49152
attention_layers.5.o_proj.bias                 [384]                  384
attention_layers.5.vec_proj.weight            [384, 128]                49152
attention_layers.5.dk_proj.weight             [128, 50]                 6400
attention_layers.5.dk_proj.bias                 [128]                  128
attention_layers.5.dv_proj.weight             [384, 50]                19200
attention_layers.5.dv_proj.bias                 [384]                  384
attention_layers.6.layernorm.weight                 [128]                  128
attention_layers.6.layernorm.bias                 [128]                  128
attention_layers.6.q_proj.weight            [128, 128]                16384
attention_layers.6.q_proj.bias                 [128]                  128
attention_layers.6.k_proj.weight            [128, 128]                16384
attention_layers.6.k_proj.bias                 [128]                  128
attention_layers.6.v_proj.weight            [384, 128]                49152
attention_layers.6.v_proj.bias                 [384]                  384
attention_layers.6.o_proj.weight            [384, 128]                49152
attention_layers.6.o_proj.bias                 [384]                  384
attention_layers.6.vec_proj.weight            [384, 128]                49152
attention_layers.6.dk_proj.weight             [128, 50]                 6400
attention_layers.6.dk_proj.bias                 [128]                  128
attention_layers.6.dv_proj.weight             [384, 50]                19200
attention_layers.6.dv_proj.bias                 [384]                  384
attention_layers.7.layernorm.weight                 [128]                  128
attention_layers.7.layernorm.bias                 [128]                  128
attention_layers.7.q_proj.weight            [128, 128]                16384
attention_layers.7.q_proj.bias                 [128]                  128
attention_layers.7.k_proj.weight            [128, 128]                16384
attention_layers.7.k_proj.bias                 [128]                  128
attention_layers.7.v_proj.weight            [384, 128]                49152
attention_layers.7.v_proj.bias                 [384]                  384
attention_layers.7.o_proj.weight            [384, 128]                49152
attention_layers.7.o_proj.bias                 [384]                  384
attention_layers.7.vec_proj.weight            [384, 128]                49152
attention_layers.7.dk_proj.weight             [128, 50]                 6400
attention_layers.7.dk_proj.bias                 [128]                  128
attention_layers.7.dv_proj.weight             [384, 50]                19200
attention_layers.7.dv_proj.bias                 [384]                  384
               out_norm.weight                 [128]                  128
                 out_norm.bias                 [128]                  128
        post_lin_list.0.weight             [64, 128]                 8192
          post_lin_list.0.bias                  [64]                   64
        post_lin_list.1.weight               [1, 64]                   64
          post_lin_list.1.bias                   [1]                    1
--------------------------------------------------------------------------
Total params: 1734629
Trainable params: 1734629
Non-trainable params: 0
Attempting to load checkpoint...
Recent checkpoint loaded successfully.
Settings: 
{'dataset': {'additional_attributes': ['forces', 'stress'],
             'data_format': 'json',
             'dataset_device': 'cpu',
             'name': 'MP_data_forces',
             'num_workers': 0,
             'prediction_level': 'graph',
             'preprocess_params': {'all_neighbors': True,
                                   'cutoff_radius': 8.0,
                                   'edge_calc_method': 'ocp',
                                   'edge_dim': 50,
                                   'n_neighbors': 250,
                                   'node_dim': 100,
                                   'node_representation': 'onehot',
                                   'num_offsets': 2,
                                   'preprocess_edge_features': False,
                                   'preprocess_edges': False,
                                   'preprocess_node_features': True,
                                   'self_loop': True},
             'processed': True,
             'pt_path': 'data/mp_data_forces/',
             'src': '/global/cfs/projectdirs/m3641/Shared/Materials_datasets/MP_data_forces/raw/data.json',
             'target_path': None,
             'test_ratio': 0.0,
             'train_ratio': 0.95,
             'transforms': [{'args': {'index': -1},
                             'name': 'GetY',
                             'otf': True}],
             'val_ratio': 0.05,
             'verbose': True},
 'model': {'activation': 'silu',
           'aggr': 'add',
           'attn_activation': 'silu',
           'dropout_rate': 0.0,
           'gradient': True,
           'hidden_channels': 128,
           'model_ensemble': 1,
           'name': 'torchmd_etEarly',
           'num_heads': 8,
           'num_layers': 8,
           'num_post_layers': 1,
           'num_rbf': 50,
           'otf_edge_attr': True,
           'otf_edge_index': True,
           'otf_node_attr': False,
           'pool': 'global_add_pool',
           'pool_order': 'late',
           'post_hidden_channels': 64,
           'prediction_level': 'graph'},
 'optim': {'batch_size': 32,
           'batch_tqdm': False,
           'clip_grad_norm': 10,
           'loss': {'loss_args': {'weight_energy': 0.01,
                                  'weight_force': 50.0,
                                  'weight_stress': 50.0},
                    'loss_type': 'ForceStressLoss'},
           'lr': 0.0001,
           'max_checkpoint_epochs': 0,
           'max_epochs': 400,
           'optimizer': {'optimizer_args': {}, 'optimizer_type': 'AdamW'},
           'scheduler': {'scheduler_args': {'factor': 0.8,
                                            'min_lr': 1e-05,
                                            'mode': 'min',
                                            'patience': 10,
                                            'threshold': 0.0002},
                         'scheduler_type': 'ReduceLROnPlateau'},
           'verbosity': 1},
 'submit': False,
 'task': {'checkpoint_path': 'results/07/torchmd-5/checkpoint_0/checkpoint.pt',
          'continue_job': True,
          'identifier': 'matstructpredict-torchmd-5',
          'load_training_state': True,
          'log_id': '2024-02-19-09-19-10-796',
          'model_save_frequency': 1,
          'output_frequency': 1,
          'parallel': False,
          'run_mode': 'train',
          'save_dir': None,
          'seed': 26876824,
          'use_amp': True,
          'write_output': ['val']},
 'trainer': 'property'}
Starting regular training
Running for 50 epochs on TorchMD_ET model
Epoch: 0350, Learning Rate: 0.000017, Training Error: 1.73275, Val Error: 4.12694, Time per epoch (s): 1731.18067
Epoch: 0351, Learning Rate: 0.000017, Training Error: 1.73986, Val Error: 4.03841, Time per epoch (s): 1699.89551
Epoch: 0352, Learning Rate: 0.000017, Training Error: 1.73339, Val Error: 4.17014, Time per epoch (s): 1698.60602
Epoch: 0353, Learning Rate: 0.000017, Training Error: 1.75373, Val Error: 3.98510, Time per epoch (s): 1697.46050
Epoch: 0354, Learning Rate: 0.000017, Training Error: 1.73085, Val Error: 4.13452, Time per epoch (s): 1700.40396
Epoch: 0355, Learning Rate: 0.000017, Training Error: 1.74003, Val Error: 3.91809, Time per epoch (s): 1699.23037
Epoch: 0356, Learning Rate: 0.000017, Training Error: 1.71180, Val Error: 4.16973, Time per epoch (s): 1697.19612
Epoch: 0357, Learning Rate: 0.000017, Training Error: 1.74009, Val Error: 4.00841, Time per epoch (s): 1698.64127
Epoch: 0358, Learning Rate: 0.000017, Training Error: 1.73424, Val Error: 4.19271, Time per epoch (s): 1698.12922
Epoch: 0359, Learning Rate: 0.000017, Training Error: 1.74353, Val Error: 4.04027, Time per epoch (s): 1699.99709
Epoch: 0360, Learning Rate: 0.000017, Training Error: 1.73421, Val Error: 4.11662, Time per epoch (s): 1698.07628
Epoch: 0361, Learning Rate: 0.000017, Training Error: 1.73396, Val Error: 4.17029, Time per epoch (s): 1698.58086
Epoch: 0362, Learning Rate: 0.000017, Training Error: 1.74917, Val Error: 4.23564, Time per epoch (s): 1697.81680
Epoch: 0363, Learning Rate: 0.000017, Training Error: 1.73121, Val Error: 4.11868, Time per epoch (s): 1698.08799
Epoch: 0364, Learning Rate: 0.000017, Training Error: 1.73072, Val Error: 4.22588, Time per epoch (s): 1697.54366
Epoch: 0365, Learning Rate: 0.000017, Training Error: 1.71691, Val Error: 3.95595, Time per epoch (s): 1699.11801
Epoch: 0366, Learning Rate: 0.000017, Training Error: 1.75371, Val Error: 4.34149, Time per epoch (s): 1704.51969
Epoch: 0367, Learning Rate: 0.000017, Training Error: 1.71941, Val Error: 4.01046, Time per epoch (s): 1702.76954
Epoch: 0368, Learning Rate: 0.000013, Training Error: 1.69230, Val Error: 4.07614, Time per epoch (s): 1697.84525
Epoch: 0369, Learning Rate: 0.000013, Training Error: 1.66876, Val Error: 4.19892, Time per epoch (s): 1698.80866
Epoch: 0370, Learning Rate: 0.000013, Training Error: 1.69038, Val Error: 4.22057, Time per epoch (s): 1697.89199
Epoch: 0371, Learning Rate: 0.000013, Training Error: 1.69271, Val Error: 4.29156, Time per epoch (s): 1704.50101
Epoch: 0372, Learning Rate: 0.000013, Training Error: 1.68073, Val Error: 4.17940, Time per epoch (s): 1698.72943
Epoch: 0373, Learning Rate: 0.000013, Training Error: 1.67985, Val Error: 4.07704, Time per epoch (s): 1701.68545
Epoch: 0374, Learning Rate: 0.000013, Training Error: 1.69106, Val Error: 3.96792, Time per epoch (s): 1704.11264
Epoch: 0375, Learning Rate: 0.000013, Training Error: 1.67210, Val Error: 4.13930, Time per epoch (s): 1699.89021
Epoch: 0376, Learning Rate: 0.000013, Training Error: 1.67955, Val Error: 3.99315, Time per epoch (s): 1699.70438
Epoch: 0377, Learning Rate: 0.000013, Training Error: 1.68554, Val Error: 4.18621, Time per epoch (s): 1703.35533
Epoch: 0378, Learning Rate: 0.000013, Training Error: 1.66662, Val Error: 4.12263, Time per epoch (s): 1703.77068
Epoch: 0379, Learning Rate: 0.000013, Training Error: 1.67486, Val Error: 4.11894, Time per epoch (s): 1699.59840
Epoch: 0380, Learning Rate: 0.000013, Training Error: 1.66861, Val Error: 3.94227, Time per epoch (s): 1698.51905
Epoch: 0381, Learning Rate: 0.000013, Training Error: 1.67700, Val Error: 4.05105, Time per epoch (s): 1698.39354
Epoch: 0382, Learning Rate: 0.000013, Training Error: 1.67753, Val Error: 4.10332, Time per epoch (s): 1697.75665
Epoch: 0383, Learning Rate: 0.000013, Training Error: 1.69388, Val Error: 4.13516, Time per epoch (s): 1699.28266
Epoch: 0384, Learning Rate: 0.000013, Training Error: 1.66342, Val Error: 4.15530, Time per epoch (s): 1697.36858
Epoch: 0385, Learning Rate: 0.000013, Training Error: 1.66974, Val Error: 4.11777, Time per epoch (s): 1700.41580
Epoch: 0386, Learning Rate: 0.000013, Training Error: 1.65735, Val Error: 4.07963, Time per epoch (s): 1698.31534
Epoch: 0387, Learning Rate: 0.000013, Training Error: 1.67314, Val Error: 4.08393, Time per epoch (s): 1698.17896
Epoch: 0388, Learning Rate: 0.000013, Training Error: 1.68417, Val Error: 4.03471, Time per epoch (s): 1698.64986
Epoch: 0389, Learning Rate: 0.000013, Training Error: 1.67704, Val Error: 3.87787, Time per epoch (s): 1699.15632
Epoch: 0390, Learning Rate: 0.000013, Training Error: 1.66279, Val Error: 4.12640, Time per epoch (s): 1703.94745
Epoch: 0391, Learning Rate: 0.000013, Training Error: 1.66322, Val Error: 4.02161, Time per epoch (s): 1701.07212
Epoch: 0392, Learning Rate: 0.000013, Training Error: 1.68000, Val Error: 4.21770, Time per epoch (s): 1698.27949
Epoch: 0393, Learning Rate: 0.000013, Training Error: 1.66390, Val Error: 4.02022, Time per epoch (s): 1708.52447
Epoch: 0394, Learning Rate: 0.000013, Training Error: 1.66826, Val Error: 4.13564, Time per epoch (s): 1716.53162
Epoch: 0395, Learning Rate: 0.000013, Training Error: 1.66285, Val Error: 4.33494, Time per epoch (s): 1714.81159
Epoch: 0396, Learning Rate: 0.000013, Training Error: 1.65367, Val Error: 4.20150, Time per epoch (s): 1704.02433
Epoch: 0397, Learning Rate: 0.000013, Training Error: 1.66735, Val Error: 3.98374, Time per epoch (s): 1704.43394
Epoch: 0398, Learning Rate: 0.000013, Training Error: 1.65138, Val Error: 4.03106, Time per epoch (s): 1703.47783
Epoch: 0399, Learning Rate: 0.000013, Training Error: 1.67510, Val Error: 4.04024, Time per epoch (s): 1704.30024
Final Losses: 
Saved val error: 4.06363
